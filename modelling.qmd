---
title: "Modelling"
---

The word “modelling” is used broadly to describe the process of analysis involved in data science. In this guide, a ‘model’ is a simplified quantitative representation of a real world situation, which often answers a specific question. Modelling can be used to help derive insights or drive decision making.

## Keep in mind

* Apply an equity lens all the way through the model lifecycle: consider who is benefiting from a model, who is vulnerable, and who is excluded. This lens should inform the data chosen, how the data is processed, algorithm choice, how the model is evaluated, and how the model is used
* Closely engage with the stakeholders and data providers (see [End-user engagement](collaboration.qmd#end-user-engagement))
* Maintain clinician engagement throughout 
* Have good quality documentation to share the work with others (see [Documentation and management](data-access.qmd#data-management))
* Version control the code base so that work is reproducible (see [Documentation and management](data-access.qmd#data-management))
* Check data quality and availability including operational concerns, sanity checking and bias (see [Data quality](data-preparation.qmd#data-quality))
* Consider your audience when presenting data (see Transparency, interpretability and explanation).

This guidance should be applicable to any analytical technique.

## Modelling process

Although the details can be different case by case, there are several general steps and a few principles that may help to achieve good results.

[Modelling process overview](img/modelling-diagram.png){fig-alt="A flowchart of the modelling process"}
Adapted from W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)

Scoping should clearly define what problem a model is intended to solve. This affects almost all the later steps. In general, a more specific scope is more likely to lead to success, which should also be defined at the outset. Documenting the scope usually involves sign-off from oversight groups, and can reduce wasted effort or misunderstanding.

## Modelling approaches

One general principle of modelling design is to select models that are as simple as possible while capable of providing solutions to the defined problem.

There are often multiple modelling approaches suitable for solving one problem, and each has advantages and disadvantages. If capacity and feasibility allow, it is often beneficial to utilise more than one technique - similar insights provided via different approaches add to the credibility of modelling.

The choice of technique is influenced by your ability to be able to explain how it works to end users and operationalise the inputs to the model. Consider this prior to choosing which model is used.

A non-exhaustive list of categories of quantitative models and the example healthcare problems that they are used for:

| Model | Example use case |
|-------|------------------|
| Descriptive statistics | Understand disease prevalence and demographics of patient cohorts |
| Regression model | Relationship between risk factors vs. cost of cancer treatment |
| Classification model | Identify high risk patients |
| Clustering model | Identify different patient cohorts |
| Simulations | Workforce planning during a pandemic using simulated disease spread trend |
| Optimisation | Minimising surgeons' overtime while planning as effectively as possible to meet surgery demands |
| System dynamics | GP training plan for the next ten years given the population and workforce dynamics |
| Language models | Model-aided auto coding of clinical documents |

For more models in healthcare, please refer to W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010).

## Documentation and management

Documenting all the steps and experiments is highly recommended. Specific information about the software used in model development and the modelling environment itself should be comprehensively documented, including the language or software used, dependencies, and version numbers, to enable the model to be able to be reproduced. Frequently a package is updated and causes a change that affects the production, performance, or other aspects of a model.

For machine learning model management, there are open-source tools such as [MLflow](https://mlflow.org/) to efficiently manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Tools including **pipenv/renv** assist portability and reproducibility, and help generate this documentation.

Providing an explicit statement of model assumptions to analysts or end users e.g. input parameter values chosen, uncertainty in those values, deterministic vs stochastic modelling, sensitivity analyses.

Use of a version control system such as **git** is very important for coordinating collaboration, avoiding rework, auditing, and generally maintaining your own sanity.

## Modelling tools

Commonly used tools for statistical analysis and modelling that require minimum programming:

* Statistical Package for the Social Sciences (SPSS). Suitable for descriptive statistics, parametric and non-parametric analyses, and visualisation of analysis results.

* Microsoft Excel. Easy to have some initial observation over tabular data, suitable to generate summary metrics of data, and create simple data visualisations. Excel has some notable drawbacks including:

* A limitation for showing up to 1,048,576 rows by 16,384 columns in a worksheet

* As an artefact, it is difficult to audit and rerun with different data

* The data and analytics layers are entangled, which can compromise data integrity.

* Cloud based modelling platforms such as BigML ([https://bigml.com/](https://bigml.com/)). These platforms provide a web interface for users to easily run machine learning modelling experiments as well as interpretability and collaborative features. However, users may need to upload their data to the cloud, which may require additional data policy checking for the use case.

Commonly used programming tools for statistical analysis and modelling:

* R. R is a fully open-source software environment for statistical computing and graphics. R has a GUI Rstudio, where R markdown can be a plug-in to facilitate R code and markdown based report/documentation generation and thus collaboration. The R community is active in developing and maintaining various analysing and modelling packages. Essential packages for data science includes:

  * For data manipulation: dplyr, tidyr, dbplyr

  * For modelling: caret, e1071, mlr3

  * For visualisation: ggplot2, plotly

  * For report generation: knitr

  * For creating interactive web interface: shiny

* Python. Python is a high-level object-oriented programming language with good code readability. Python has a comprehensive ecosystem to support common statistical analysis, machine learning, deep learning and the implementation and deployment of these data science features. Jupyter notebook/lab is a widely used tool in the Python ecosystem for shareable data science work. Many dependencies in the Python ecosystem are open-source. Essential python dependencies for data science include:

  * For data manipulation: pandas, numpy

  * For statistical analysis: scipy, statsmodels

  * For modelling including machine learning: scikit-learn, statsmodels

  * For deep learning: tensorflow, pytorch, keras

  * For visualisation: matplotlib, seaborn, plotly, bokeh

  * For creating interactive web interface: dash, streamlit

* SAS. SAS is a statistical software suite for data management, advanced analytics, multivariate analysis, predictive analytics and so on. SAS has its own GUI for non-technical users but programming via the SAS language can provide more flexibility and functionality in the analysis. It provides free SAS Ondemand for Academics (previous University Edition) for non commercial users such as students and educators.

* Matlab. Matlab is a programming language and environment optimised for numeric computing. Especially for matrix manipulation, Matlab has advantages compared with the aforementioned other tools. For machine learning and deep learning, Matlab provides the Statistics and Machine Learning Toolbox and the Deep Learning Toolbox, respectively.

Open source tools such as R and python provide transparency, collaboration and a low barrier to entry. Commercial tools tend to have a more sophisticated user experience, but a limited ecosystem, and provide some assurances (e.g. extension/library support) that aren't available through commercial partners.

## Experiment tracking tools

The process of developing a model is iterative, potentially involving trial and error for experiment configuration tuning, training and evaluation of the model. When the complexity of a model grows, monitoring experiment configurations and model performance could become a hassle. Using automatic experiment tracking tools such as Tensorboard, weights & biases are helpful to overcome those issues. In general, experiment tracking tools provide features such as visualisation, logging, integration with multiple ML frameworks and model profiling. Also, such tools provide integration facilities for CI/CD pipelines, ML lifecycle management tools which helps to streamline the model training and deployment pipeline while keeping traces in the experiment lifecycle.

There are lists online which suggest experiment tracking tools for machine learning [note an example list](https://neptune.ai/blog/best-ml-experiment-tracking-tools).

## Model validation

### Data partitioning

Models are usually tuned to the dataset that they are trained with. Using independent datasets for validating the performance of a model is preferred. However, in the healthcare domain, data sharing can make this difficult.

Where independent datasets are not available, the model needs to be validated using the development dataset. There are two main approaches for train-test data splitting and model validation.[^1]

1. Cross validation

This method mixes and shuffles all the data points and splits them into k folds (usually 5-10) for iterative performance measuring. In each iteration, one fold is left out as the testing set and the others are used as the training set. By these iterations, k measures of the chosen performance metric(s) could be obtained to assess the model's average performance and its variance, which is used as an estimate of the model's performance on unseen data.

[![Example of K-Fold Cross Validation when k=5](/img/kfolds.png){fig-alt="Example of k-fold cross validation"}](http://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png)

To use this method, it is important to make sure that there are no significant data changes along the time within the model development dataset, and no foreseeable data changes between the development set and the future data that the model would be applied on.

In healthcare, it is also important to check whether the model will be used at an event level or patient level in advance.

There could be multiple events for the same patient in a dataset and if no proper consideration is taken into account while splitting the testing data (or other preprocessing like deduplication), this could potentially lead to information leakage and inaccuracy.

For example, in a longitudinal dataset of health encounters, the dataset could be split to group all of a patient's encounters together (so these are not split across sets), or split by time (which could mean a patient's encounters are split between different validation sets). Consider whether these types of splitting will make a meaningful difference to your validation exercises. Also be mindful of anything that involves a flow or transfer.

1. Time-wise validation

Many healthcare model applications have in nature a time dimension, as data are collected by healthcare events that happen across time. There could be a trend in the data as people age, population structure changes, and the healthcare technologies/systems evolve.
>
To better estimate the future performance of a model, a time-wise splitting and validating approach can be taken. This method sets data of a specified period along time as the testing set and uses the data before the testing set as training data. A visualisation of performance change over time will be obtained after a number of iterations, which provides an estimation of the trend of the model's performance change in the near future - will it be relatively stable, gradually decreasing or increasing.

Some models have complex hyperparameter space and require an additional split of data for early stop in training or additional hyperparameter optimisation. In classification models that rely on a cut-off threshold in application (e.g. a high risk patient identification model based on numeric risk scores), the cut-off threshold can also be seen as part of the model hyperparameters, and its determination should be considered as part of the model development. Hence the performance validation process should be using a partition of data that is not used in either model training, additional hyperparameter optimisation, or the threshold optimisation in particular.

In all cases, care should be taken to ensure that the sampling unit is complete. For instance, we may be modelling a flow of events, in which case the sampling unit is likely to be a patient. If the modelled data consists of multiple records per patient, it will be important to ensure that complete patient records are sampled.

### Performance metrics

The type of performance metric used will depend on the model being evaluated and the context of the project. For models that predict a value, such as a linear regression model, R-squared and root mean square error (RMSE) are common.

In health, metrics calculated from a confusion matrix for models that output a class (e.g. a decision tree) or probability (e.g. logistic regression) are commonly used. To produce a confusion matrix for a model that outputs a probability, a probability threshold is applied (values above the threshold are positive and values below are negative). The probability threshold can be set to optimise a particular metric, can be set by other optimisation techniques, or otherwise set according to the use case. Seek clinical input on the appropriate thresholds as they can impact patient care.

Common performance metrics used can include the following. Note that not all are appropriate for each model or data type or question being answered.

* **Precision** quantifies the proportion of positive class predictions that actually belong to the positive class.

* **Recall** quantifies the number of positive class predictions made out of all positive examples in the dataset.

* **Model accuracy** is a machine learning classification model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations.

* **F-Measure** provides a single score that balances both the concerns of precision and recall in one number.

* **Positive and negative predictive values** (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively. The PPV and NPV describe the performance of a diagnostic test or other statistical measure.

  * Positive predictive value (PPV) and negative predictive value (NPV) are best thought of as the clinical relevance of a test

* **Prevalence** is the number of cases in a defined population at a single point in time and is expressed as a decimal or a percentage.

* **Sensitivity** is the percentage of true positives (e.g. 90% sensitivity = 90% of people who [have]{.underline} the target disease will test positive).

* **Specificity** is the percentage of true negatives (e.g. 90% specificity = 90% of people who [do not have]{.underline} the target disease will test negative).

* **AUROC** is [a performance metric for "discrimination"](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673738/): it tells you about the model's ability to discriminate between cases (positive examples) and non-cases (negative examples.) For a ranking use case AUC-ROC is a measure that indicates how good the model is at ranking cases based on a score (how likely any two cases are correctly ordered). 0.5 indicates that the model is no better than random at ranking and 1 indicates a perfect model.

* **AUC-PRC** is a measure that indicates how good the model is at minimising the tradeoff between precision and recall. A high AUC-PRC represents the model can achieve high recall and high precision at the same time.

## Further resources

[Rules of Machine Learning: Best Practices for ML Engineering](https://developers.google.com/machine-learning/guides/rules-of-ml#overview) (Google Machine Learning Guides)

Erdemir, A., Mulugeta, L., Ku, J.P. *et al.* [Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective.](https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02540-4) *J Transl Med* 18, 369 (2020). [https://doi.org/10.1186/s12967-020-02540-4](https://doi.org/10.1186/s12967-020-02540-4)

W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)

[A Practical Guide to Maintaining Machine Learning in Production](https://eugeneyan.com/writing/practical-guide-to-maintaining-machine-learning/)
