# Data preparation

## Data manipulation/wrangling

Good data manipulation/wrangling practice is based on comprehensive knowledge of the data. We recommend the following:

* For a given data source, learn as much as you can about its collection, storage, how it is updated and maintained, the definition and dependency of each data item, and the limitations. There are more upfront ethical considerations when dealing with health data.

* For local data collection, consider how to store it (in files, in a database, locally or on cloud etc.) to make information retrieval easier, or/and data sharing easier. See the Data Management section.

* Consider the size of the data - should it be processed all at once, batch processing or stream processing? Your computing location (on your computer, on a local server, or on a cloud provider) may affect your decision here.

* Consider if the data needs de-identification. See [Data identifiability](#data-identifiability).

* Maintain an equity lens for any type of data science work, including building and evaluating models. See [Bias in data](#bias-in-data).

* Involve clinical leads when data wrangling. Within healthcare, the risk associated with some data (e.g. laboratory data or vitals) is often distributed at the extreme ends. It is difficult for data scientists to know if the risk is distributed linearly without the clinical context.

### Data quality

Do a data quality review as an early step and produce statistical summaries for sanity checks. Consider additional rounds of quality reviews and involve the data owners/managers if the data is complex.

Data profiling (for example, with [Pandas-profiling](https://github.com/ydataai/pandas-profiling)) is recommended for both data quality checking and data understanding. Data profiling involves providing descriptive summaries and visualisations of the data for review with clinicians and subject matter experts.

Basic aspects for data quality check include:

* Data availability across time

* Data distribution change across time

* Outliers, do we need cap variables with extreme values?

* Data completeness/missingness

* Duplication in the data

* Target label distribution (data imbalance)

* Association between features and the target

* Data timeliness: The data resource provides the data in appropriate time.

* Scalability: The data can be accessed from many components without losing its meaning

* Provenance: The data should be based on valid authority

* Locality: The location of data resource should be provided to check relevance of data

* Structure: form of data

* Adoption: The data are useful for the project purpose and adaptable with the requirements, including operational requirements (will the data be refreshed, timely and in the expected format?)

* Identification of extreme/outlier values, and variables that require capping.

## Data linkage

Many projects that involve health data will require linking datasets on keys. One example is where individual health data is linked to census data based on location. Another example is linking data across government departments or agencies, for instance linking hospitalisation data to vaccination data, matching on NHI.

Records with a single identifier, and those with multiple identifiers, can have equity considerations in health and indicate interaction with the health system; e.g., duplicative data where a person has been issued multiple NHI identifiers.

The best practice is to link datasets prior to de-identification, so that the linkage can be as robust as possible and de-identification can minimise the risk of re-identification.

### How to link

Where datasets need to be deidentified prior to linking at the level of the individual or patient, you will need to consider how to maintain the integrity of the linkage. Unique identifiers, such as NHI, may need to be encrypted, rather than suppressed, in the process of de-identification. Where data comes from different sources, unique identifiers will need to be encrypted in the same way in order to link datasets. If the processed data is to be delivered back to the health provider at the level of the individual, then there needs to be the possibility of decryption of the identifiers. Orion Health's De-identifier product will create a secure mapping for this purpose.

### Risk of re-identification

Two datasets that separately will not identify an individual may identify an individual once linked. As such prior to linking datasets, the risk of re-identification of an individual should be considered. Ideally, datasets are linked together and de-identified prior to delivery from the health provider, however this is often not possible.

### Approaches

Where data cannot be linked on a unique identifier, there are approaches that can be used to match records. The deterministic approach is to match on a combination of attributes that will uniquely identify a person. An issue with this approach is that in a dataset that is de-identified well, it should be difficult to uniquely identify individuals based on a combination of unique attributes.

An alternative probabilistic approach is to calculate conditional probabilities to determine the likelihood that a given pair of records match.

## Missing data

The missing data may provide context in the absence and presence of records. For example missing hospitalisation records might indicate access to care - either you may not have access or you are healthy and haven't needed to see your GP. Take care in interpreting this as it can be hard to be sure in health. This is why having clinical engagement in a project is so important, as is stepping back to see the wider picture

Missing data is a common problem with most health data sets. When you encounter missing data, you can choose how to manage it, such as:

* Remove it

* Interpret it as "not applicable"

* Impute it (fill it in with other values)

Often a combination of removing excessively missing (according to some threshold) observations/variables and then imputing the remaining missing values is effective.

Your decision should be based on whether your method is tolerant of missing values, whether the data appears to be missing completely at random (MCAR) in which case it could be ignored, or whether there are some patterns in its missingness.

If removing the data, you'll need to decide whether to remove variables or observations (e.g. rows or columns).

Try proxy variables or alter data collection processes to avoid missing data as much as possible.

Most data is unlikely to be MCAR. Often, data is more likely to be missing for particular reasons (missing not at random; MNAR). Consider if there is a reason for the missingness, as this can itself be informative. For example, data may be missing from smaller subgroups within the dataset. This is particularly important when considering stratification for ethnicity. It's important to consider the potential bias introduced into a model by removing missing data or the impact on equity if data is removed. Before continuing with imputation or removal, check for patterns in the data of individuals who have partial missing data to assess the equity and bias implications of removing or imputing it. Look for evidence of non-random missingness by comparing the complete and missing data groups through stratification for important demographic variables such as age and ethnicity.

Imputation can be a useful technique for overcoming missing data problems, but can be computational intensive. Review Prof Thomas Lumely's guide on imputation for more information about the application of imputation techniques. Your strategy for handling missing values will have implications for how you handle future unseen data too.

## Use of synthetic data

Synthetic data can be an option if there is difficulty in accessing real-world datasets, whether this be due to timing, privacy concerns, access problems, lack of participation in healthcare by certain groups, or rarity of a disease. Synthetic data may also satisfy a use case (e.g. for data augmentation purposes).

Synthetic data can be used for prototyping or building analytics dashboards that are ready to plug in to real-world datasets.

Synthetic data can also be used for machine learning purposes in the sensitive health domain, including to integrate data relating to under-represented conditions and groups of people, and to protect privacy. However, off-the-shelf models that generate synthetic data (such as [Synthea](https://synthetichealth.github.io/synthea/)) are not mature and should be carefully assessed for local use. It is highly recommended that the user is aware of how the data is generated and what are the limitations.

In lieu of building or enhancing models with synthetic data, off-the-shelf calculators could be considered, with the caveat that calculators are ideally subject to local validation. Orion Health has developed the Algorithm Hub which provides a shared knowledge-base of pre-trained models, algorithms and risk calculators which were reviewed by a multidisciplinary governance group.

Reference: [https://www.nature.com/articles/s41551-021-00751-8](https://www.nature.com/articles/s41551-021-00751-8)
