[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Guide to Health Data Science in Aotearoa New Zealand",
    "section": "",
    "text": "This guide collects a set of positive practices for health data science projects in Aotearoa New Zealand, summarising knowledge from the experiences of researchers from the Precision Driven Health partnership and the wider health and data science sectors in this area.\nIt is a living document, not a finished product. Contributions from all researchers are welcome. You can contribute by editing a page. Pages are Markdown files in a public GitHub repository rendered using Quarto."
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This guide is a collection of learnings based on the experience of New Zealand health data science researchers and other contributors. It is aimed at practitioners and those that oversee their work, providing practical advice without repeating publicly available resources and references (which are instead linked to). For data scientists, the guide also assumes a basic understanding of data science techniques, addressing the context and principles of how these are applied to lead to a successful outcome.\nThe guide is not intended to be a detailed technical reference on particular data science tool sets or clinical concepts.\nEarlier sections outline the context and setup of a health data science project; later sections introduce the technical aspects of data science including modelling."
  },
  {
    "objectID": "01-intro.html#about-precision-driven-health",
    "href": "01-intro.html#about-precision-driven-health",
    "title": "1  Introduction",
    "section": "1.2 About Precision Driven Health",
    "text": "1.2 About Precision Driven Health\nPrecision Driven Health (PDH) is an award-winning partnership between Aotearoa New Zealand’s health information technology (IT) sector, health providers and universities, aimed at improving health outcomes through data science.\nPDH seeks to increase data science capability in New Zealand’s health sector and encourage innovation in the use of health data."
  },
  {
    "objectID": "01-intro.html#contributors-and-reviewers",
    "href": "01-intro.html#contributors-and-reviewers",
    "title": "1  Introduction",
    "section": "1.3 Contributors and reviewers",
    "text": "1.3 Contributors and reviewers\nWe would like to thank the people who contributed to compiling and reviewing this guide:\n\nAlex Kazemi\nCK Jin\nDuncan Croft\nEdmond Zhang\nFleur Armstrong\nIvan Rivera\nJamal Zolhavarieh\nJuliet Rumball-Smith\nKelly Atkinson\nKevin Ross\nLuke Boyle\nNing Hua\nPieta Brown\nQuan Sun\nRachel Owens\nTom Gutteridge\nVipula Dissanayake"
  },
  {
    "objectID": "02-starting.html",
    "href": "02-starting.html",
    "title": "2  How do I start?",
    "section": "",
    "text": "Much like other aspects of business or research, you’ll probably need to convince at least one other person that your idea is important and worthy of the time or resources you need to pursue it. After navigating these hurdles and producing a model or other analysis, your carefully crafted tool will need care and periodic review to stay functional and relevant.\nIf you are coming from a clinical background, you might appreciate some of the context around understanding the data, and its preparation for modelling.\nIf you are approaching this from a data science background, the unique health data landscape will be important to appreciate.\nIf you are looking for a more general introduction to health data science in general, keep reading from here!"
  },
  {
    "objectID": "03-basics.html",
    "href": "03-basics.html",
    "title": "3  Understanding the basics",
    "section": "",
    "text": "Data science guides often include the concept of a lifecycle. Examples of these more general guides include the CRoss Industry Standard Process for Data Mining (CRISP-DM) and the Microsoft Team Data Science Process (TDSP).\n\n\n\nThe CRISP-DM Data Science Lifecycle\n\n\nThis guide aligns broadly with the life cycle stages outlined below, but your project may follow a different path - this is fine! The list below will get you started in thinking about the broad steps of your project:\n\nBusiness understanding – What does the health system or business or world need?\nData acquisition and understanding – What data do we have? What do we need? Is it ‘clean’ (free of incorrect, incomplete, or duplicate data)?\nData preparation – How do we organise the data for analysis, including modelling?\nModelling – What modelling techniques should we apply?\nEvaluation – Which model best meets the health system or business or world’s objectives?\nDeployment – How do users and stakeholders access the results?\nMaintenance - How will a model be monitored and refreshed over time?"
  },
  {
    "objectID": "03-basics.html#research-vs-audit-vs-bau",
    "href": "03-basics.html#research-vs-audit-vs-bau",
    "title": "3  Understanding the basics",
    "section": "3.2 Research vs audit vs BAU",
    "text": "3.2 Research vs audit vs BAU\nPeople and organisations undertaking health data science may need help clarifying whether what they want to do is “research” or not. There are many definitions of research, but a simple definition is:\nResearch creates new knowledge, which could include new methods or processes, and could lead to the creation of new guidelines.\nBusiness as usual (BAU) includes activities such as development, sales, and support. If your business includes the creation of new capabilities or features, this portion of your activities can be perceived as research.\nAudits are activities that check whether you are following existing processes or guidelines. Testing new processes becomes research, since you are departing from the existing processes.\nQuality improvement seeks to improve an existing process that is already of benefit to patients. Machine learning (ML) models incorporated into workflows can cause changes to the standard of care patients receive, including unintended consequences. Translation from a tool to implementation is essentially research to validate and determine the impact of ML on patients.\nResearch projects have different goals, such as ‘analysis only’ versus ‘analyse and model for future use’. Some projects may have a commercial focus on delivering an outcome for use in practice. It is important to define what you are doing as clearly as possible at the start of a project (refer to Social licence & consent).\nAny research undertaken in New Zealand using health data requires ethical review. See ‘Ethics and privacy’."
  },
  {
    "objectID": "03-basics.html#clearly-articulate-the-goal",
    "href": "03-basics.html#clearly-articulate-the-goal",
    "title": "3  Understanding the basics",
    "section": "3.3 Clearly articulate the goal",
    "text": "3.3 Clearly articulate the goal\nGood data science doesn’t have to be complicated, but it should be clear. Many projects fail to clearly articulate their goal which may lead to people working to a different agenda.\nCo-design - a design-led process that uses creative and participatory methods involving all stakeholders to ensure the result meets their needs - is essential at this early stage. It’s important to be explicit in these circumstances; you cannot define a problem, determine the appropriate data and methodology, interpret results or run a successful project without the input and partnership of the end-users and other stakeholders.\nDefine goals up front and consider, what question is being answered? Do you want to answer a question that is relevant to a specific population (often geographically defined), or to produce a model or other output that can be generalised to other populations? Is it a proof of concept where further work may be required, or does it require implementation to be used in practice?\nOften the real goal is masked by sub-goals. A project that has been forced to fit within a call for proposals or programme, but the actual goals of the researcher, and those that the project is set up to address, are different is one example of this. Carefully consider the problem you are trying to solve and seek external validation to confirm if it is a real-world problem for end users."
  },
  {
    "objectID": "03-basics.html#choosing-the-right-question---exploring-feasibility-and-delivering-value",
    "href": "03-basics.html#choosing-the-right-question---exploring-feasibility-and-delivering-value",
    "title": "3  Understanding the basics",
    "section": "3.4 Choosing the right question - exploring feasibility and delivering value",
    "text": "3.4 Choosing the right question - exploring feasibility and delivering value\nChoosing the right question is an essential part of any research effort.\nBefore beginning, get really clear on the problem that is being solved and understand if it is clinically meaningful and adds value. Define the need or problem and then find data/technology to answer it, not the other way around. For model development, define what success looks like e.g how accurate should a model be.\nCo-design is essential - seek information from different stakeholders (clinicians, patients/consumers, systems users) and research the existing evidence/base or literature on the topic. Simple frameworks like the ‘5 Whys’ can be helpful to apply. The end-users and those impacted by the use of a tool need to understand and believe in the value being delivered to drive engagement and improve translation into real world applications.\nMapping the clinical workflow into which a model fits in dynamic contexts is also useful - do clinicians have current workarounds for problems or barriers that remain useful, or is there a genuine gap ? Can the model or algorithm be responsive to real world complexity?\nConsider the wider health sector and if there is willingness and need for uptake. Value can be provided through: improvement in patient outcomes or experience, improving efficiency, reduction of uncertainty, prioritising resources to those most in need, reduction in resource requirements.\nIn building a business case, the value of data science projects needs to be expressed from the perspectives of each of the different stakeholders. Consider the unique selling points, unintended consequences, risks vs benefits from social, economic, health outcomes and reputational perspectives. Articulate value based on each stakeholder’s perspective. Consider engaging with a health economist for specialist advice. Do not exaggerate or over-sell the potential real-life impact or underplay potential risks.\nWork through the use-case end-to-end before starting, getting clear on how the algorithm will be accessed and used, and what decisions will be made as a result.\nWhat level of transparency or interpretability is required in order for the results to be useful? Do we only care about predictions or are we looking to drive process change?\nConsider how the work will be validated or trialled in a clinical setting, e.g. as a clinical trial or passive background comparison with production data.\nAlgorithms and models are only one piece of the puzzle in improving health outcomes - what are the interventions in place or available to take action based on ML-driven insights? For example, if someone is identified as being at higher risk of readmission to hospital,what can be offered as a result?\nIs an algorithm necessarily the best solution? Consider this Nature article on Steps to avoid overuse and misuse of machine learning in clinical research"
  },
  {
    "objectID": "03-basics.html#funding",
    "href": "03-basics.html#funding",
    "title": "3  Understanding the basics",
    "section": "3.5 Funding",
    "text": "3.5 Funding\nFinding funding to pursue a data science project can be difficult. In some cases, data science endeavours may be supported by a specific public or private organisation.\nIn New Zealand, funding tends to be available for conceptual research, or for commercialisation activities. In our experience, there is substantial translational work required between these two phases. Potential sources for funding translative work include Callaghan Innovation, the MedTech Innovation Quarter (MedTech-iQ), and/or individual organisations.\nIt is usually necessary to pitch, propose, or otherwise justify a request for funding data science activities. You might be responding to a third party’s request for proposals (RFP) or open call for a funding round. When considering the project you want to do, think about the points below.\nWho is paying for this work to be completed? What is their interest? Who has an interest in the outcome, including the researchers? Does their influence/interest in the outcome need to be declared or managed in any particular way?\nRecognising and declaring conflicts of interest is essential to understanding the applicability of the data science results, and any biases that may apply.\nIf this work is successful, what further investment will be required to ensure that the work leads to its full potential?\nIf the ultimate goal is a commercial product, there is often substantial investment required to take a proof of concept to a maintainable, robust product. Think about:\nWho will provide support to the people using this product, even if they are using it only for research purposes or in informal ways? What is the lifecycle of the research output? Who will be able to look after it in 5 years’ time?"
  },
  {
    "objectID": "03-basics.html#legal-ip-and-regulatory-considerations",
    "href": "03-basics.html#legal-ip-and-regulatory-considerations",
    "title": "3  Understanding the basics",
    "section": "3.6 Legal, IP, and regulatory considerations",
    "text": "3.6 Legal, IP, and regulatory considerations\nLegal advice should be sought, and issues related to legal, intellectual property and/or regulatory issues explicitly discussed and documented prior to starting work. Consider who holds the data you want to use, who owns the models and what may be required for lifecycle management.\nSoftware may be considered a medical device if it is used in the diagnosis, treatment, prevention, cure, or mitigation of diseases or other conditions. Depending on how the intended use is defined, it may be subject to country dependent regulatory requirements. Specialised regulatory advisors should be engaged early as the process takes time and documentation for approval may need to be generated during the development process."
  },
  {
    "objectID": "04-collaboration.html",
    "href": "04-collaboration.html",
    "title": "4  People, capability & collaboration",
    "section": "",
    "text": "Typical health data science projects require a multidisciplinary team. The involvement of different roles will likely fluctuate through the different stages of a project lifecycle.\nCommonly needed skills include the areas below. Some individuals will have multiple skills, but few will have everything they need.\n\nsubject matter expertise in data\nmachine learning\ndesign\nclinical\ngovernance\nconsumer/patient and specifically impacted communities\nlegal and privacy\nethics\nimplementation and change management\n\nOften you may be involved in multiple projects, and you may find it useful to share expertise across projects.\nIt is particularly important to ensure plenty of time for clinical input to data understanding and preparation for data science. Defining input features in clinically relevant ways is really important for health data science. For example, ‘cancer’ might be an important input at an individual level - does this mean currently active cancer? Within the last five years? Any exclusions? Time needs to be allocated to work through and validate these definitions with clinicians.\n\nRefer to the Section 4.2 section (covers Māori engagement, co-design)\nAdvisory groups are a good way to elicit feedback. Early in any project try to meet with a group of experts and discuss your research plan. Experts will always understand aspects of the research which need to be considered beyond what is in the data."
  },
  {
    "objectID": "04-collaboration.html#sec-end-user-engagement",
    "href": "04-collaboration.html#sec-end-user-engagement",
    "title": "4  People, capability & collaboration",
    "section": "4.2 End-user Engagement",
    "text": "4.2 End-user Engagement\nEnd users (usually consumers and clinicians) should be engaged early for co-design, and to understand how outputs can be tangible for those who will use them or be impacted by them. Refer Transparency, interpretability, and explanation. Understand workflows and where tools/models may be used. End-user engagement will also help to drive IT implementation if the benefits are clearly articulated to the appropriate stakeholders so work can be prioritised.\n\n4.2.1 Impacted customers/patients/groups\nAny recommendations for changes or improvements should be interpreted through the lens of the groups they are likely to affect, acknowledging the principle of “nothing about us without us”.\nData scientists are unlikely to have the correct context or cultural awareness to fully grasp what the data is telling them. This includes Māori and other ethnicity groups, consumers, and perspectives that cover age and ability ranges.\n\n\n4.2.2 Clinicians\nClinical engagement will often make or break a project's success - both to ensure efficacy and to evangelise the results. In general, clinicians will adopt tools and models that fit into their workflow patterns, saving time or reducing errors.\nCo-designing from the start and having a clinical champion/sponsor to ensure that developments can be incorporated in existing workflows so they can be used will set you up for success see Operational deployment.\nInterpretability (understanding the reasoning behind predictions and decisions made by the model) is also key. Important stakeholders are unlikely to have lots of time to learn about your work, so displaying it in an easy to understand manner is essential. It shouldn't be assumed that everyone will interpret the outputs in the same way, and understand what action is then required. Keep in mind that prospective customers may not be technologically or mathematically savvy."
  },
  {
    "objectID": "04-collaboration.html#collaboration",
    "href": "04-collaboration.html#collaboration",
    "title": "4  People, capability & collaboration",
    "section": "4.3 Collaboration",
    "text": "4.3 Collaboration\n\nAgree collaboration will be be supported across different organisations (who may all be using different software and systems and have limitations around what can be used).\nRole definition, responsibilities\nRegular check-ins/stand-ups are helpful\nBe mindful of individuals’ schedules and availability, particularly clinicians"
  },
  {
    "objectID": "04-collaboration.html#measuring-success",
    "href": "04-collaboration.html#measuring-success",
    "title": "4  People, capability & collaboration",
    "section": "4.4 Measuring success",
    "text": "4.4 Measuring success\n\nBe clear what success looks like. This will be different for research vs. implementation.\n\nAre you looking to learn/measure something specific?\nAre you expecting secondary or downstream benefits\n\nMaking sure the bigger picture is kept in mind - not getting lost in the detail in a way that doesn’t add value\nHave we reached the point of diminishing returns for investing further in model development?\nIf you have measurable benefits as an objective, you should develop a benefits realisation plan"
  },
  {
    "objectID": "04-collaboration.html#project-management",
    "href": "04-collaboration.html#project-management",
    "title": "4  People, capability & collaboration",
    "section": "4.5 Project management",
    "text": "4.5 Project management\n\nData science is an iterative process of development as we progressively learn more about the data, relationships in the data and how effective modelling is.\nGood project management or product development practices should be applied to data science projects, while accepting that these projects are often experimental or exploratory in nature.\nData issues flow ‘downstream’ and can impact every other part of a project. Ensure that sufficient time is allocated upfront to review and correct data quality (this also often happens in cycles - the analysis reveals quirks in the data that can be explored further and corrected).\nProject management should strive for continuous visibility, demonstrable progress. How are we tracking against timelines, budget, and the desired outcome?\nCreate open feedback channels where possible\n\nThink about how to develop mockups/prototypes as early as possible for feedback - can you start with a very simple model, Excel dashboard or static design to help ensure that what you are developing will deliver value?\n\nHealthcare data science projects will often span multiple organisations\nDocumenting failures and lessons learnt helps prevent repeating mistakes.\nTechnology for progress tracking: Consider\n\nJira,\na spreadsheet-based activity tracker,\nTrello board,\nemailed summaries of actions and next steps"
  },
  {
    "objectID": "04-collaboration.html#governance",
    "href": "04-collaboration.html#governance",
    "title": "4  People, capability & collaboration",
    "section": "4.6 Governance",
    "text": "4.6 Governance\nGood governance is fundamentally concerned with the value and risk of a project, and needs to be established to ensure accountability and oversight. Developing policies and procedures are a way of managing risk and clearly articulating principles of accountability, transparency, ethical use, privacy, and consent.\nYou should define and tailor a governance approach based on the needs of your own organisation.\nUsually a data science project is undertaken within a wider programme of work, which has its own governance structure and processes. Occasionally, more substantial initiatives will require their own standalone governance.\nGovernance may be applied at different levels, such as governance of data (inputs) or governance of models (product/output). Those involved in model governance should come from diverse demographic and technical backgrounds, including perspectives of consumers and/or those who are impacted by the outputs of your work. Those involved in data governance need to have an understanding of data flows - how data is captured, stored and used.\nQuestions to ask:\n\nAre ethics applications required?\nWhat existing governance groups and/or processes would need to be involved?\nWhich policies need to be followed?\nWhat level of documentation is required throughout?\nWill this be shared publicly? If so, how?\nWho is responsible for signing off?\nIs a new governance structure or process required here?\nWhat maintenance and/or ongoing review may be required? NICE Evidence Standards Framework for Digital Health Technologies has a helpful list of points to consider\n\nThere are many resources for governance groups and executive teams who are looking to write data policies and procedures. These include:\n\nNew Zealand Data and Information Management Principles (open data NZ)\nThe government’s open data policies and best-practice guidance for agencies managing how data is stored, published and used.\nNational Ethical Standards for Health and Disability Research (NEAC). These standards set out the ethical requirements for researchers, health service providers and disability service providers and apply whether or not research or quality improvement activities require review by an ethics committee.\nData protection and Privacy Act 1993\nHealth Information Privacy Code 1994\nHISO 10064:2017 Health Information Governance Guidelines - Manatu Hauora Guidance to the health and disability sector on the safe sharing of health information\nNew Zealand Government Open Access and Licensing framework (NZGOAL) - For those who work for a government agency and want to enable appropriate re-use of your agency’s material by licensing its copyright works or releasing non-copyright material for re-use.\nWhat New Zealand people expect from guidelines for data use and sharing, Findings from public engagement February/March 2017 - Data Future Partnership, A Path to Social Licence: Guidelines for Trusted Data Use (original is gone; refer to https://www.toiaria.org/our-projects/our-data-our-way/)\nLayered model for AI governance a conceptual framework for thinking about governance for AI.\nLessons learned from developing a COVID-19 algorithm governance framework in Aotearoa New Zealand Practical considerations from a governance group\nAlgorithm Charter\nHealth and Disability Ethics Committees\nTe Mana Rauranga\nMinistry of Health - Emerging Health Technology Advice & Guidance"
  },
  {
    "objectID": "05-data-landscape.html",
    "href": "05-data-landscape.html",
    "title": "5  The unique health data landscape",
    "section": "",
    "text": "Access to healthcare data requires a lot more up-front consideration than data from other industries.\nNB HQSC framework"
  },
  {
    "objectID": "05-data-landscape.html#finding-data",
    "href": "05-data-landscape.html#finding-data",
    "title": "5  The unique health data landscape",
    "section": "5.1 Finding data",
    "text": "5.1 Finding data\nData for health data science projects is everywhere! There is no shortage of available data in New Zealand. However, keep the following concepts in mind when you are considering how to find data, or access data you may already be interested in:\n\nData can only be used for the purpose for which it was collected; any other use is called “secondary purpose” and requires additional consent (see Use and re-use of data)\nHealth data used for research purposes needs thought around how the research will be conducted efficiently, ethically, and with privacy and safety front-of-mind.\n\n\n5.1.1 Public health data\nFor understanding population trends and context, aggregated data sets and web tools are publicly available via the Ministry of Health and Statistics New Zealand (Stats NZ). Micro data (at the level of the individual) is available to researchers on application to National Collections or Stats NZ for Confidentialised Unit Record Files (CURFs). Dissemination of micro data is in accordance with the Privacy Act, health legislation and contracts and access is strictly controlled according to use.\nThrough a review of Aotearoa New Zealand health datasets, PDH has produced an interactive and updateable list of data sources available in New Zealand (Aotearoa NZ Data Sources Review). Visit this at data.precisiondrivenhealth.com.\n\nAotearoa NZ Data Sources Review. https://data.precisiondrivenhealth.com.\n\nVirtual Health Information Network. IDI Guides. 2 June 2017, https://vhin.co.nz/guides/.\nThe Virtual Health Information Network includes many researchers who are using the Integrated Data Infrastructure supported by Statistics NZ. VHIN’s IDI guides (Virtual Health Information Network) may be helpful in understanding what centralised data is available. The IDI is accessible through following a specific process."
  },
  {
    "objectID": "05-data-landscape.html#ethics-privacy",
    "href": "05-data-landscape.html#ethics-privacy",
    "title": "5  The unique health data landscape",
    "section": "5.2 Ethics & privacy",
    "text": "5.2 Ethics & privacy\nWhen dealing with health data, we need to be careful when assessing if a research project requires ethical approval. Ethical approval is typically required for any evidence-generating studies, or studies which propose changes to current standard of care (before any model is evaluated/validated) and should always be sought prior to accessing patient data. Approval is given through a written application process and gives permission for the research to be conducted by named investigators during a specific time period.\nMany of the questions asked in an ethics application are important in evaluating the risks and benefits and can indicate if there is social licence for the intended research. The application process forces researchers and organisations to consider whether their work has a net benefit for society (see Social license - use of health data).\nEthics approvals in New Zealand can be provided at a national level (Health and Disability Ethics Committees [HDEC]) or local level (e.g. Auckland Health Research Ethics Committee). On top of this, hospitals may have their own research offices which may require separate notification (e.g. Research Office, Hauora a Toi Bay of Plenty, Te Whatu Ora)\n\n\n\n\n\n\nImportant\n\n\n\nSeek appropriate ethics approvals prior to accessing patient data.\nThe implications of assuming that ethics is not required can have significant downstream effects on a project (e.g. reputation, delays). HDEC can provide ‘Scope of Review’ services to advise if the research you are doing is considered exempt from requiring an ethics application. Consider seeking written evidence of this as an assurance for stakeholders. See HDEC website for the current process: find out if your study requires HDEC review on the HDEC website\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSome ethics applications can take months before an approval is granted, so allow adequate time for this process, factoring when review meetings are held. There is also the potential that further questions may be asked at this review stage.\n\n\nConsider if existing patient consent is sufficient to cover use of the data.\nGenerally all use of administrative health data for research purposes will need to go through the HDEC process. Consent for use of administrative data is mainly around its use for improving the care of that individual within the health service or quality improvement processes (defined as audits or other activities). Unless the work is specifically with business intelligence within a healthcare organisation, any projects should be checked to see if ethical approval from HDEC is required.\nPrivacy Impact Assessments should also be conducted at an early stage to identify potential data protection risks on the data of the individuals included. Measures should be adopted to eliminate or mitigate risks. Independent ethical reviews could be commissioned if needed.\nHelpful references:\n\nhttp://ethicstoolkit.ai/ - A useful checklist to address ethical and governance questions\nNational Ethical Standards for Health and Disability Research (NEAC) - These standards set out the ethical requirements for researchers, health service providers and disability service providers, and apply whether or not research or quality improvement activities require review by an ethics committee.\nA Research Ethics Framework for the Clinical Translation of Healthcare Machine Learning\n\n\n5.2.1 Consent\nBefore using data - particularly data that contains sensitive information - you need consent to use it. In the General Data Protection Regulation (GDPR), a regulation in EU law on data protection and privacy, consent is defined as follows:\n\n\n\n\n\n\n“any freely given, specific, informed and unambiguous indication of the data subject’s wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.”\n\n\n\nSome health providers allow patients the opportunity to opt in for their data to be used for research and operational purposes, or to improve their care. Opting in may or may not explicitly allow for the processing of their de-identified data by third parties. More commonly, the patient has not provided explicit consent. Whether it is appropriate to use this data will depend on context (for example public good vs commercial gain), the degree to which the data is anonymised, whether data is provided in aggregated form, and the purpose the data is being used for.\n\n\n5.2.2 Use and re-use of data\n‘Use’ of data relates to using data for the purpose for which it was consented and collected. Re-use of data (or secondary use) is when we use data collected for another purpose.\nWhen we re-use data, we should be mindful of its purpose, coverage, bias, timeliness, and applicability to the secondary use. For instance, the National Minimum Data Set is gathered for policy formation, performance monitoring, research and review. It may be useful for understanding hospitalisations, but does not provide a complete picture of an individual's health journey.\n\n\n5.2.3 Social license - use of health data\nSocial licence is the implied permission to make decisions about the management and use of the public data in a way that ensures trust and confidence in the way that data is managed. Organisations who store health data are stewards. Any use of data, outside of that explicitly consented, is subject to social licence.\nConceptually, the level of engagement required to gain social license is higher for decisions that are more likely to affect an individual and are fully automated, compared to those that are manual and affect an entire population. Social licence isn’t ‘gained’ or ‘approved’; organisations need to gauge people’s thoughts, feelings, perceptions on the use of their data. These attitudes are dynamic and constantly evolving.\nAssurance to stakeholders should be provided through transparency and governance to limit reputational harm, particularly when using patient data in a commercial context. Consider what data you are handling, how identifiable it is and if patient consent is specifically required. This is particularly important for data that may be re-used for secondary purposes - the original consent should be carefully reviewed. Data should be used for public good in a way that is equitable, with consideration for any unintended consequences, such as increased clinician workload (due to workflow disruption), potential for misuse of the model, and perpetuating biases that exist in the data.\nData needs to be treated with care and a suitable data management plan can help.\nIn our experience, privacy, confidentiality, security, transparency, communication and the purpose for which data is used, are often raised as concerns by clinicians, administrators, legal experts, and other stakeholders.\nUseful resources:\n\nA Path to Social Licence Guidelines for Trusted Data Use - Data Futures Partnership 2017\nResearch on social licence for health data re-use Manatu Hauora research providing context about the re-use of health data and social licence."
  },
  {
    "objectID": "05-data-landscape.html#understand-the-data",
    "href": "05-data-landscape.html#understand-the-data",
    "title": "5  The unique health data landscape",
    "section": "5.3 Understand the data",
    "text": "5.3 Understand the data\nKnowing the data you’re able to access is critical to understanding what is possible, in addition to understanding the question or problem you are trying to solve.\n\n\n\n\n\n\nTip\n\n\n\nHealthcare data is often complex and ‘dirty’ (inaccurate, incomplete or inconsistent). When possible, liaise with analysts who work within the organisation to gain a local understanding of the data.\n\n\nHealth data is often only available after a significant lag time, and with a slow refresh rate. For example, there is a specific chain of events that lead to updates to national health data collections, and this can take a significant amount of time - often many months.\nAt an early stage, it is valuable to consider:\n\nData landscape - What data is collected? How is access managed? Does it help address the problem you are trying to solve?\nCharacteristics - what is the format, type & size? When was the data collected? How often will you receive it?\nHow much historical data is available, and what is the quality? Use the minimum necessary!\nWhat purpose was it collected for, and does that influence your interpretation?\nConsent - Is the use of data in this covered by existing patient consent?\nData collection, maintenance, publication - Distinguish between data already collected and new data created by the study. How do you plan to maintain/publish this data?\nPersonally identifiable information - Is de-identification required? Who will do this? Refer to Handling personal health information\nData availability - how long will it take to source and are there any reporting or system lags?\nLabelling/annotation - does the project need Human In the Loop (HITL) Mechanism for annotating and validating the input and output data?\n\nIn some cases, publicly accessible datasets may be a useful starting point. Be sure to read the access requirements linked to using these.\nData sources, including New Zealand and publicly available:\n\nPDH’s references to data sources used in health.\nWhere can I find health information? | Ministry of Health NZ Data sources commonly used when analysing the health of New Zealand populations.\nManatu Hauora - Health statistics and data sets"
  },
  {
    "objectID": "05-data-landscape.html#accessing-data",
    "href": "05-data-landscape.html#accessing-data",
    "title": "5  The unique health data landscape",
    "section": "5.4 Accessing data",
    "text": "5.4 Accessing data\nData access can be one of the more tricky and time consuming aspects of a data science project. Access should be addressed at a very early stage. ​​It can take a lot of time to understand what data is available, agree on access, create a data sharing agreement, and to receive the data.\nAny project that involves access to individual-level healthcare data from an organisation will need upfront discussion with the organisation around existing data governance, privacy impacts and ethics applications. This discussion will include data sovereignty and consideration of equity. Once these points have been clearly delineated, the process of how the data is to be handled should be much clearer.\n\n5.4.1 Planning data access\nTo understand data access requirements we must consider the sensitive nature of health data, consent, security and technical needs.\nKnow what you are asking for. Your data request should be well specified and considered and it will take some time, discussion with the health data provider and analysis to formulate.\nMany providers will have strict data transfer and storage protocols, however these may not always be followed by individuals. How the data will be transferred and stored, and at what frequency should be discussed and agreed, before a .csv file suddenly lands in your inbox.\nData that contains Protected Health Information (PHI) or Personal Identifiable Information (PII) cannot be freely shared, however after de-identification (see section Data Identifiability) it may be possible to share such data, under certain terms.\n\n\n5.4.2 Data sharing agreements\n\n\n\n\n\n\nTip\n\n\n\nData sharing agreements should be written and agreed early, ideally guided by legal advice.\n\n\nData sharing agreements between a data provider and data recipient typically cover:\n\nWhat data will be shared and for how long\nWhat is the consented use of the data\nHow the data will be shared and stored\nHow the data will be used and its destruction after project completion\nPrivacy and confidentiality\nHow security will be ensured\nHow compliance with and breaches of the agreement will be handled\nAnything else relevant, such as de-identification of data\n\nThese agreements cannot escape data protection and privacy laws, and agreements that already exist between a data provider and a patient. This should be taken into consideration before any data is transferred.\n\n\n5.4.3 The data request\nAnalysis and discussion regarding the data request are also vital at an early stage. Health data is sensitive and describes people in their most personal and vulnerable moments. It is a privilege to be working with it, so be specific with your request and don’t seek more than is really needed.\nSome questions to consider are:\n\nWhat data sources are available and accessible?\nWhat fields are needed?\nCan the data be operationalised?\nWhat time period is to be covered?\nHow do privacy, ethics and equity considerations affect my data request?\nWhat is the minimum data set that I will need for this project?\nWho has the ability to provide the data?\n\n\n\n5.4.4 Waiting….and more waiting\nOnce you have your plan in place, smooth sailing isn’t guaranteed. De-identification, data sharing agreements and data requests can take time. But even when an agreement is signed and plans are ready, you might find a lot more time passes while waiting for data.\nHealth data providers usually have to undertake a lot of work before sending data in the form agreed. Data may be located in different systems, extraction may require booking in technical resources and the data may need pre-processing and annotation. Also, while you may be eagerly awaiting the data, it may not be afforded the same priority by a busy health provider. Patience is a virtue, but the occasional gentle nudge may be necessary. You also need to manage your own workload, and you don’t want all of your data buses arriving at once.\n\n\n5.4.5 Transferring data\nSome providers may require that you use their systems, whether on premises or in the cloud, so that the data never ‘leaves’ their organisation. Others may be happy to upload their data to a secure cloud-based server provided by you, and some may have their own file transfer systems.\nTry to avoid email for data transfer, even if a file is password protected. Emailed data can be difficult to track and audit. This may result in multiple copies being saved using up storage capacity. Files are also frequently too large to be attached. Security is a particular concern, especially where either sender or recipient uses a third-party email service – you do not know how many servers are handling that message.\nIf you will be receiving data regularly, it’s also better to minimise the number of transfers by having data sent in batches. Any data you receive should also be different from what you have already received. Have this conversation with the health data provider at the planning stage.\nFinally, please ensure you confirm successful receipt of the data."
  },
  {
    "objectID": "05-data-landscape.html#data-sovereignty",
    "href": "05-data-landscape.html#data-sovereignty",
    "title": "5  The unique health data landscape",
    "section": "5.5 Data sovereignty",
    "text": "5.5 Data sovereignty\nData sovereignty refers to the understanding that data is subject to the laws of the nation within which it is collected and stored. In New Zealand, there is also a focus on where the data is stored and processed. Data agreements should take care to address these points so they are clear to all parties.\nMāori data sovereignty recognises that Māori data should be subject to Māori governance. Māori data sovereignty supports tribal sovereignty and the realisation of Māori and iwi aspirations. Māori must be included in any work about Māori data. An equity lens is required and ideally should include Māori in the research team as well as in external review/advisor roles.\nUseful resources on data sovereignty include:\n\nTe Mana Raraunga (Māori Data Sovereignty Network) has helpful resources and guidance.\nHRC also has Guidelines for Researchers on Health Research Involving Māori."
  },
  {
    "objectID": "05-data-landscape.html#data-governance-and-data-management",
    "href": "05-data-landscape.html#data-governance-and-data-management",
    "title": "5  The unique health data landscape",
    "section": "5.6 Data governance and Data management",
    "text": "5.6 Data governance and Data management\nData governance and data management are distinct concepts with some areas of overlap and linkage. Both areas concern how we access, store, log, secure, maintain, use, share and destroy data in an efficient and standardised way, guided by principles such as accountability, transparency, ethical use, privacy, consent and data availability, integrity and quality.\n\n\n\n\n\n\n\nData governance\nData management\n\n\n\n\nMaintains oversight, provides accountability, shapes and communicates policies and procedures\nEnacts policies and procedures for day-to-day handling of data\n\n\nGovernance group or executive team or board; a diverse group of stakeholders to cover multiple perspectives\nIndividual data stewards\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis is written from the perspective of a consulting practice, research organisation or any organisation that uses data which it does not directly collect.\n\n\n\n5.6.1 Data Governance\nFor more information about Governance see the Governance section.\n\n\n5.6.2 Data Management\n\n5.6.2.1 Storage and security\nWith a data access plan in place, and an understanding of what is available, appropriate storage and security requirements need to be in place.\nWhen storing data on premises or on your secure cloud-based service, there should be a clear process for access control. Larger, regular batches of data are preferable, but where there is irregularity in timing and peaks in volume, cloud based auto-scaled storage may help keep costs down.\nOn-premises data storage should be on a secure server, and not stored or replicated on personal computers.\n\n\n5.6.2.2 Data register or log\nWhat starts out as a data trickle can quickly become a data flood. It is worthwhile maintaining a register of data from the first file received.\nA data register or log can help you or your organisation track:\n\nwhat data you have received\nwhen you received it\nwhere you are storing it\nwho can/should have access to it\nwhen it should be destroyed\nwhat processed data sets exist and where, particularly if they contain PHI\n\nThe format can be tailored to suit your organisation (for example, using a spreadsheet).\n\n\n5.6.2.3 Tools for data management\nManaging data for a small organisation or research team could be as simple as having a secure storage solution and a spreadsheet to keep track of important metadata. Larger organisations will likely need formal data governance, data policies in place and data stewards to manage the use, storage and security of data. Tailor your approach based on the data being handled.\nConsider version control for all aspects of the project, including data; depending on the tooling that you use, this may already be an included capability. DVC is one tool designed specifically for data version control for machine learning. Your development environment or cloud storage solution may also include features of this type. Data files can be named with versions, however this method is not as robust as automated versioning that is managed by a tool.\nData management tools are responsible for carrying a wide range of data or documents."
  },
  {
    "objectID": "06-data-identifiability.html",
    "href": "06-data-identifiability.html",
    "title": "6  Data Identifiability",
    "section": "",
    "text": "It is essential that the sensitive nature of health data is considered throughout the life cycle of the project from the initial stages of idea generation through to data transfer, receipt, preparation, and modelling. For this reason, we’ve highlighted here the important processes to think about."
  },
  {
    "objectID": "06-data-identifiability.html#what-is-sensitive-information-pii-and-phi",
    "href": "06-data-identifiability.html#what-is-sensitive-information-pii-and-phi",
    "title": "6  Data Identifiability",
    "section": "6.1 What is sensitive information (PII and PHI)?",
    "text": "6.1 What is sensitive information (PII and PHI)?\nThe Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a US federal law that required the creation of national standards to protect sensitive patient health information from being disclosed without the patient’s consent or knowledge. The definitions provided by HIPAA are important to consider even when you are not working in the United States or with data from the USA.\nThe HIPAA defines PII and PHI as:\n\n\n\nPII - Personally identifiable information\nPHI - Protected Health Information\n\n\n\n\nAny piece of information that can be traced to an individual’s identity, not necessarily health related (e.g. address).\n​​Any piece of information in an individual’s medical record that was created, used, or disclosed during the course of diagnosis or treatment that can be used to personally identify them. HIPAA has a detailed definition of PII.\n\n\n\n\n\n\n\n\n\nInfo\n\n\n\n\n​​Name\nAddress (including subdivisions smaller than state such as street address, city, county, or zip code)\nAny dates (except years) that are directly related to an individual, including birthday, date of admission or discharge, date of death, or the exact age of individuals older than 89\nTelephone number\nFax number\nEmail address\nSocial Security number (in the USA)\nMedical record number or NZ National Health Index (NHI) number\nHealth plan beneficiary number\nAccount number\nCertificate/license number\nVehicle identifiers, serial numbers, or license plate numbers\nDevice identifiers or serial numbers\nWeb URLs\nIP address\nBiometric identifiers such as fingerprints or voice prints\nFull-face photos\nAny other unique identifying numbers, characteristics, or codes\n\n\n\nYour organisation may have a different name or definition. However it is named or defined, it’s important to note that health data contains sensitive information that patients would not want disclosed. This data is a privilege to use and should be treated with utmost care.\nWhen a provider transfers data, the first task is to check whether you should be in receipt of that data. This requires procedures for checking data for PII and PHI. You should also have procedures in place for what to do should you find any PII or PHI, including the destruction of the data, and reporting of the incident.\nIf a health data provider has provided you access to PHI or PII containing data in contravention of a data sharing agreement, continued access to or retention of that data is not defensible by reasoning that the mistake was theirs. A guiding principle is that appropriate storage, transfer and use of data is the responsibility of all parties involved."
  },
  {
    "objectID": "06-data-identifiability.html#checking-your-data-for-the-presence-of-identifiable-information",
    "href": "06-data-identifiability.html#checking-your-data-for-the-presence-of-identifiable-information",
    "title": "6  Data Identifiability",
    "section": "6.2 Checking your data for the presence of identifiable information",
    "text": "6.2 Checking your data for the presence of identifiable information\nIn general, you are most likely to be working with de-identified data. The data provider is responsible for ensuring that data released is compliant and the data receiver also has a responsibility for highlighting if this has not been done.\nIt is imperative to check that any dataset you receive meets the de-identification standard you are expecting; e.g. you have not been sent a dataset that contains identifiers when it should not.\nAs a minimum, do a common sense check of metadata and manually scan the first 1000 rows of a dataset for any PHI e.g. unique identifiers, address, and name.\nTools which can be of help with this process include:\n\nDe-identifier (Orion Health) - can identify PII and remove it, though we note the de-identification step should be the responsibility of the data provider\nMacie (Amazon): https://aws.amazon.com/macie/ - able to check data stored in Amazon Web Services (AWS)\nComprehend Medical (Amazon) - provides a PHI detection API\nDLP (Google): https://cloud.google.com/dlp"
  },
  {
    "objectID": "06-data-identifiability.html#de-identifying-structured-data",
    "href": "06-data-identifiability.html#de-identifying-structured-data",
    "title": "6  Data Identifiability",
    "section": "6.3 De-identifying structured data",
    "text": "6.3 De-identifying structured data\nDe-identification can be achieved through the suppression or transformation of certain identifying attributes. For example, a health data provider can be asked to exclude name fields, provide age range rather than date of birth, transform address to a statistical area unit, and encrypt unique identifiers prior to data transfer.\nEven with suppression or transformation of individual identifiers, individuals can still be identified in structured data where information about an individual is already known. For instance, if you know someone who is 57, has had breast cancer and lives in a certain postcode, if only one person in a dataset has these attributes, this information could be used to identify that person in the de-identified data. Particularly when linked to other data sources, more personal information could then be gleaned for that person.\nk-anonymisation or ε-differential privacy techniques can be applied to ensure that individuals cannot be identified via combinations of attributes. Both techniques involve a tradeoff of privacy and data utility.\n\nk-anonymisation is where at least k individuals share an identifying set of attributes for any individual. It can be achieved through suppression of attributes or the generalisation of values (for instance using age ranges).\nε-differential privacy involves adding noise to the original distribution in a way that ensures that the probability that a statistical query will produce a given result is nearly the same on a dataset that has had one person’s information removed. The higher the level of de-identification, the more noise is added to the distribution."
  },
  {
    "objectID": "06-data-identifiability.html#de-identifying-unstructured-data",
    "href": "06-data-identifiability.html#de-identifying-unstructured-data",
    "title": "6  Data Identifiability",
    "section": "6.4 De-identifying unstructured data",
    "text": "6.4 De-identifying unstructured data\nIt is estimated that approximately 80% of health data is stored as unstructured text. Unstructured data poses a particular challenge for anonymisation. Basic pattern matching using regular expressions may go some way to locating identifying text and can be useful for finding attributes with known formats (for example NHI or date of birth). However, free text, even in very short phrases, can contain a sometimes surprising amount of other identifying information. Natural language processing techniques and machine learning can be applied for more sophisticated textual de-identification, with the support of de-identification tools."
  },
  {
    "objectID": "07-data-preparation.html",
    "href": "07-data-preparation.html",
    "title": "7  Data preparation",
    "section": "",
    "text": "Good data manipulation/wrangling practice is based on comprehensive knowledge of the data. We recommend the following:\n\nFor a given data source, learn as much as you can about its collection, storage, how it is updated and maintained, the definition and dependency of each data item, and the limitations. There are more upfront ethical considerations when dealing with health data.\nFor local data collection, consider how to store it (in files, in a database, locally or on cloud etc.) to make information retrieval easier, or/and data sharing easier. See the Data Management section.\nConsider the size of the data - should it be processed all at once, batch processing or stream processing? Your computing location (on your computer, on a local server, or on a cloud provider) may affect your decision here.\nConsider if the data needs de-identification. See Data identifiability.\nMaintain an equity lens for any type of data science work, including building and evaluating models. See Bias in data.\nInvolve clinical leads when data wrangling. Within healthcare, the risk associated with some data (e.g. laboratory data or vitals) is often distributed at the extreme ends. It is difficult for data scientists to know if the risk is distributed linearly without the clinical context.\n\n\n\nDo a data quality review as an early step and produce statistical summaries for sanity checks. Consider additional rounds of quality reviews and involve the data owners/managers if the data is complex.\nData profiling (for example, with Pandas-profiling) is recommended for both data quality checking and data understanding. Data profiling involves providing descriptive summaries and visualisations of the data for review with clinicians and subject matter experts.\nBasic aspects for data quality check include:\n\nData availability across time\nData distribution change across time\nOutliers, do we need cap variables with extreme values?\nData completeness/missingness\nDuplication in the data\nTarget label distribution (data imbalance)\nAssociation between features and the target\nData timeliness: The data resource provides the data in appropriate time.\nScalability: The data can be accessed from many components without losing its meaning\nProvenance: The data should be based on valid authority\nLocality: The location of data resource should be provided to check relevance of data\nStructure: form of data\nAdoption: The data are useful for the project purpose and adaptable with the requirements, including operational requirements (will the data be refreshed, timely and in the expected format?)\nIdentification of extreme/outlier values, and variables that require capping."
  },
  {
    "objectID": "07-data-preparation.html#data-linkage",
    "href": "07-data-preparation.html#data-linkage",
    "title": "7  Data preparation",
    "section": "7.2 Data linkage",
    "text": "7.2 Data linkage\nMany projects that involve health data will require linking datasets on keys. One example is where individual health data is linked to census data based on location. Another example is linking data across government departments or agencies, for instance linking hospitalisation data to vaccination data, matching on NHI.\nRecords with a single identifier, and those with multiple identifiers, can have equity considerations in health and indicate interaction with the health system; e.g., duplicative data where a person has been issued multiple NHI identifiers.\nThe best practice is to link datasets prior to de-identification, so that the linkage can be as robust as possible and de-identification can minimise the risk of re-identification.\n\n7.2.1 How to link\nWhere datasets need to be deidentified prior to linking at the level of the individual or patient, you will need to consider how to maintain the integrity of the linkage. Unique identifiers, such as NHI, may need to be encrypted, rather than suppressed, in the process of de-identification. Where data comes from different sources, unique identifiers will need to be encrypted in the same way in order to link datasets. If the processed data is to be delivered back to the health provider at the level of the individual, then there needs to be the possibility of decryption of the identifiers. Orion Health’s De-identifier product will create a secure mapping for this purpose.\n\n\n7.2.2 Risk of re-identification\nTwo datasets that separately will not identify an individual may identify an individual once linked. As such prior to linking datasets, the risk of re-identification of an individual should be considered. Ideally, datasets are linked together and de-identified prior to delivery from the health provider, however this is often not possible.\n\n\n7.2.3 Approaches\nWhere data cannot be linked on a unique identifier, there are approaches that can be used to match records. The deterministic approach is to match on a combination of attributes that will uniquely identify a person. An issue with this approach is that in a dataset that is de-identified well, it should be difficult to uniquely identify individuals based on a combination of unique attributes.\nAn alternative probabilistic approach is to calculate conditional probabilities to determine the likelihood that a given pair of records match."
  },
  {
    "objectID": "07-data-preparation.html#missing-data",
    "href": "07-data-preparation.html#missing-data",
    "title": "7  Data preparation",
    "section": "7.3 Missing data",
    "text": "7.3 Missing data\nThe missing data may provide context in the absence and presence of records. For example missing hospitalisation records might indicate access to care - either you may not have access or you are healthy and haven’t needed to see your GP. Take care in interpreting this as it can be hard to be sure in health. This is why having clinical engagement in a project is so important, as is stepping back to see the wider picture\nMissing data is a common problem with most health data sets. When you encounter missing data, you can choose how to manage it, such as:\n\nRemove it\nInterpret it as “not applicable”\nImpute it (fill it in with other values)\n\nOften a combination of removing excessively missing (according to some threshold) observations/variables and then imputing the remaining missing values is effective.\nYour decision should be based on whether your method is tolerant of missing values, whether the data appears to be missing completely at random (MCAR) in which case it could be ignored, or whether there are some patterns in its missingness.\nIf removing the data, you’ll need to decide whether to remove variables or observations (e.g. rows or columns).\nTry proxy variables or alter data collection processes to avoid missing data as much as possible.\nMost data is unlikely to be MCAR. Often, data is more likely to be missing for particular reasons (missing not at random; MNAR). Consider if there is a reason for the missingness, as this can itself be informative. For example, data may be missing from smaller subgroups within the dataset. This is particularly important when considering stratification for ethnicity. It’s important to consider the potential bias introduced into a model by removing missing data or the impact on equity if data is removed. Before continuing with imputation or removal, check for patterns in the data of individuals who have partial missing data to assess the equity and bias implications of removing or imputing it. Look for evidence of non-random missingness by comparing the complete and missing data groups through stratification for important demographic variables such as age and ethnicity.\nImputation can be a useful technique for overcoming missing data problems, but can be computational intensive. Review Prof Thomas Lumely’s guide on imputation for more information about the application of imputation techniques. Your strategy for handling missing values will have implications for how you handle future unseen data too."
  },
  {
    "objectID": "07-data-preparation.html#use-of-synthetic-data",
    "href": "07-data-preparation.html#use-of-synthetic-data",
    "title": "7  Data preparation",
    "section": "7.4 Use of synthetic data",
    "text": "7.4 Use of synthetic data\nSynthetic data can be an option if there is difficulty in accessing real-world datasets, whether this be due to timing, privacy concerns, access problems, lack of participation in healthcare by certain groups, or rarity of a disease. Synthetic data may also satisfy a use case (e.g. for data augmentation purposes).\nSynthetic data can be used for prototyping or building analytics dashboards that are ready to plug in to real-world datasets.\nSynthetic data can also be used for machine learning purposes in the sensitive health domain, including to integrate data relating to under-represented conditions and groups of people, and to protect privacy. However, off-the-shelf models that generate synthetic data (such as Synthea) are not mature and should be carefully assessed for local use. It is highly recommended that the user is aware of how the data is generated and what are the limitations.\nIn lieu of building or enhancing models with synthetic data, off-the-shelf calculators could be considered, with the caveat that calculators are ideally subject to local validation. Orion Health has developed the Algorithm Hub which provides a shared knowledge-base of pre-trained models, algorithms and risk calculators which were reviewed by a multidisciplinary governance group.\nReference: https://www.nature.com/articles/s41551-021-00751-8"
  },
  {
    "objectID": "08-modelling.html",
    "href": "08-modelling.html",
    "title": "8  Modelling",
    "section": "",
    "text": "We use the word “modelling” broadly, to capture the process of analysis involved in data science. In this guide, a model is a simplified quantitative representation of a real world situation, often to answer a specific question. Modelling can be used as a tool to help derive insights or drive decision making. In other industries, life experience is often enough to guide you. However, in a health context domain expertise is extremely important e.g. what defines a hospitalisation?\nThis guidance should be applicable to any analytical technique."
  },
  {
    "objectID": "08-modelling.html#modelling-process",
    "href": "08-modelling.html#modelling-process",
    "title": "8  Modelling",
    "section": "8.1 Modelling process",
    "text": "8.1 Modelling process\nAlthough the details can be different case by case, there are several general steps and a few principles that may help to achieve good results.\nModelling process overview Adapted from W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)\nScoping should clearly define what problem a model is intended to solve. This affects almost all the later steps. In general, a more specific scope is more likely to lead to success, which should also be defined at the outset. Documenting the scope usually involves sign-off from oversight groups, and can reduce wasted effort or misunderstanding."
  },
  {
    "objectID": "08-modelling.html#modelling-approaches",
    "href": "08-modelling.html#modelling-approaches",
    "title": "8  Modelling",
    "section": "8.2 Modelling approaches",
    "text": "8.2 Modelling approaches\nOne general principle of modelling design is to select models that are as simple as possible while capable of providing solutions to the defined problem.\nThere are often multiple modelling approaches suitable for solving one problem, and each has advantages and disadvantages. If capacity and feasibility allow, it is often beneficial to utilise more than one technique - similar insights provided via different approaches add to the credibility of modelling.\nThe choice of technique is influenced by your ability to be able to explain how it works to end users and operationalise the inputs to the model. Consider this prior to choosing which model is used.\nA non-exhaustive list of categories of quantitative models and the example healthcare problems that they are used for:\n\n\n\n\n\n\n\nModel\nExample use case\n\n\n\n\nDescriptive statistics\nUnderstand disease prevalence and demographics of patient cohorts\n\n\nRegression model\nRelationship between risk factors vs. cost of cancer treatment\n\n\nClassification model\nIdentify high risk patients\n\n\nClustering model\nIdentify different patient cohorts\n\n\nSimulations\nWorkforce planning during a pandemic using simulated disease spread trend\n\n\nOptimisation\nMinimising surgeons’ overtime while planning as effectively as possible to meet surgery demands\n\n\nSystem dynamics\nGP training plan for the next ten years given the population and workforce dynamics\n\n\nLanguage models\nModel-aided auto coding of clinical documents\n\n\n\nFor more models in healthcare, please refer to W. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)."
  },
  {
    "objectID": "08-modelling.html#documentation-and-management",
    "href": "08-modelling.html#documentation-and-management",
    "title": "8  Modelling",
    "section": "8.3 Documentation and management",
    "text": "8.3 Documentation and management\nDocumenting all the steps and experiments is highly recommended. Specific information about the software used in model development and the modelling environment itself should be comprehensively documented, including the language or software used, dependencies, and version numbers, to enable the model to be able to be reproduced. Frequently a package is updated and causes a change that affects the production, performance, or other aspects of a model.\nFor machine learning model management, there are open-source tools such as MLflow to efficiently manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Tools including pipenv/renv assist portability and reproducibility, and help generate this documentation.\nProviding an explicit statement of model assumptions to analysts or end users e.g. input parameter values chosen, uncertainty in those values, deterministic vs stochastic modelling, sensitivity analyses.\nUse of a version control system such as git is very important for coordinating collaboration, avoiding rework, auditing, and generally maintaining your own sanity."
  },
  {
    "objectID": "08-modelling.html#modelling-tools",
    "href": "08-modelling.html#modelling-tools",
    "title": "8  Modelling",
    "section": "8.4 Modelling tools",
    "text": "8.4 Modelling tools\nCommonly used tools for statistical analysis and modelling that require minimum programming:\n\nStatistical Package for the Social Sciences (SPSS). Suitable for descriptive statistics, parametric and non-parametric analyses, and visualisation of analysis results.\nMicrosoft Excel. Easy to have some initial observation over tabular data, suitable to generate summary metrics of data, and create simple data visualisations. Excel has some notable drawbacks including:\nA limitation for showing up to 1,048,576 rows by 16,384 columns in a worksheet\nAs an artefact, it is difficult to audit and rerun with different data\nThe data and analytics layers are entangled, which can compromise data integrity.\nCloud based modelling platforms such as BigML (https://bigml.com/). These platforms provide a web interface for users to easily run machine learning modelling experiments as well as interpretability and collaborative features. However, users may need to upload their data to the cloud, which may require additional data policy checking for the use case.\n\nCommonly used programming tools for statistical analysis and modelling:\n\nR. R is a fully open-source software environment for statistical computing and graphics. R has a GUI Rstudio, where R markdown can be a plug-in to facilitate R code and markdown based report/documentation generation and thus collaboration. The R community is active in developing and maintaining various analysing and modelling packages. Essential packages for data science includes:\n\nFor data manipulation: dplyr, tidyr, dbplyr\nFor modelling: caret, e1071, mlr3\nFor visualisation: ggplot2, plotly\nFor report generation: knitr\nFor creating interactive web interface: shiny\n\nPython. Python is a high-level object-oriented programming language with good code readability. Python has a comprehensive ecosystem to support common statistical analysis, machine learning, deep learning and the implementation and deployment of these data science features. Jupyter notebook/lab is a widely used tool in the Python ecosystem for shareable data science work. Many dependencies in the Python ecosystem are open-source. Essential python dependencies for data science include:\n\nFor data manipulation: pandas, numpy\nFor statistical analysis: scipy, statsmodels\nFor modelling including machine learning: scikit-learn, statsmodels\nFor deep learning: tensorflow, pytorch, keras\nFor visualisation: matplotlib, seaborn, plotly, bokeh\nFor creating interactive web interface: dash, streamlit\n\nSAS. SAS is a statistical software suite for data management, advanced analytics, multivariate analysis, predictive analytics and so on. SAS has its own GUI for non-technical users but programming via the SAS language can provide more flexibility and functionality in the analysis. It provides free SAS Ondemand for Academics (previous University Edition) for non commercial users such as students and educators.\nMatlab. Matlab is a programming language and environment optimised for numeric computing. Especially for matrix manipulation, Matlab has advantages compared with the aforementioned other tools. For machine learning and deep learning, Matlab provides the Statistics and Machine Learning Toolbox and the Deep Learning Toolbox, respectively.\n\nOpen source tools such as R and python provide transparency, collaboration and a low barrier to entry. Commercial tools tend to have a more sophisticated user experience, but a limited ecosystem, and provide some assurances (e.g. extension/library support) that aren’t available through commercial partners."
  },
  {
    "objectID": "08-modelling.html#experiment-tracking-tools",
    "href": "08-modelling.html#experiment-tracking-tools",
    "title": "8  Modelling",
    "section": "8.5 Experiment tracking tools",
    "text": "8.5 Experiment tracking tools\nThe process of developing a model is iterative, potentially involving trial and error for experiment configuration tuning, training and evaluation of the model. When the complexity of a model grows, monitoring experiment configurations and model performance could become a hassle. Using automatic experiment tracking tools such as Tensorboard, weights & biases are helpful to overcome those issues. In general, experiment tracking tools provide features such as visualisation, logging, integration with multiple ML frameworks and model profiling. Also, such tools provide integration facilities for CI/CD pipelines, ML lifecycle management tools which helps to streamline the model training and deployment pipeline while keeping traces in the experiment lifecycle.\nThere are lists online which suggest experiment tracking tools for machine learning note an example list."
  },
  {
    "objectID": "08-modelling.html#model-validation",
    "href": "08-modelling.html#model-validation",
    "title": "8  Modelling",
    "section": "8.6 Model validation",
    "text": "8.6 Model validation\n\n8.6.1 Data partitioning\nModels are usually tuned to the dataset that they are trained with. Using independent datasets for validating the performance of a model is preferred. However, in the healthcare domain, data sharing can make this difficult.\nWhere independent datasets are not available, the model needs to be validated using the development dataset. There are two main approaches for train-test data splitting and model validation.[^1]\n\nCross validation\n\nThis method mixes and shuffles all the data points and splits them into k folds (usually 5-10) for iterative performance measuring. In each iteration, one fold is left out as the testing set and the others are used as the training set. By these iterations, k measures of the chosen performance metric(s) could be obtained to assess the model’s average performance and its variance, which is used as an estimate of the model’s performance on unseen data.\n\n\n\nExample of K-Fold Cross Validation when k=5\n\n\nTo use this method, it is important to make sure that there are no significant data changes along the time within the model development dataset, and no foreseeable data changes between the development set and the future data that the model would be applied on.\nIn healthcare, it is also important to check whether the model will be used at an event level or patient level in advance.\nThere could be multiple events for the same patient in a dataset and if no proper consideration is taken into account while splitting the testing data (or other preprocessing like deduplication), this could potentially lead to information leakage and inaccuracy.\nFor example, in a longitudinal dataset of health encounters, the dataset could be split to group all of a patient’s encounters together (so these are not split across sets), or split by time (which could mean a patient’s encounters are split between different validation sets). Consider whether these types of splitting will make a meaningful difference to your validation exercises. Also be mindful of anything that involves a flow or transfer.\n\nTime-wise validation\n\nMany healthcare model applications have in nature a time dimension, as data are collected by healthcare events that happen across time. There could be a trend in the data as people age, population structure changes, and the healthcare technologies/systems evolve. > To better estimate the future performance of a model, a time-wise splitting and validating approach can be taken. This method sets data of a specified period along time as the testing set and uses the data before the testing set as training data. A visualisation of performance change over time will be obtained after a number of iterations, which provides an estimation of the trend of the model’s performance change in the near future - will it be relatively stable, gradually decreasing or increasing.\nSome models have complex hyperparameter space and require an additional split of data for early stop in training or additional hyperparameter optimisation. In classification models that rely on a cut-off threshold in application (e.g. a high risk patient identification model based on numeric risk scores), the cut-off threshold can also be seen as part of the model hyperparameters, and its determination should be considered as part of the model development. Hence the performance validation process should be using a partition of data that is not used in either model training, additional hyperparameter optimisation, or the threshold optimisation in particular.\nIn all cases, care should be taken to ensure that the sampling unit is complete. For instance, we may be modelling a flow of events, in which case the sampling unit is likely to be a patient. If the modelled data consists of multiple records per patient, it will be important to ensure that complete patient records are sampled.\n\n\n8.6.2 Performance metrics\nThe type of performance metric used will depend on the model being evaluated and the context of the project. For models that predict a value, such as a linear regression model, R-squared and root mean square error (RMSE) are common.\nIn health, metrics calculated from a confusion matrix for models that output a class (e.g. a decision tree) or probability (e.g. logistic regression) are commonly used. To produce a confusion matrix for a model that outputs a probability, a probability threshold is applied (values above the threshold are positive and values below are negative). The probability threshold can be set to optimise a particular metric, can be set by other optimisation techniques, or otherwise set according to the use case. Seek clinical input on the appropriate thresholds as they can impact patient care.\nCommon performance metrics used can include the following. Note that not all are appropriate for each model or data type or question being answered.\n\nPrecision quantifies the proportion of positive class predictions that actually belong to the positive class.\nRecall quantifies the number of positive class predictions made out of all positive examples in the dataset.\nModel accuracy is a machine learning classification model performance metric that is defined as the ratio of true positives and true negatives to all positive and negative observations.\nF-Measure provides a single score that balances both the concerns of precision and recall in one number.\nPositive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively. The PPV and NPV describe the performance of a diagnostic test or other statistical measure.\n\nPositive predictive value (PPV) and negative predictive value (NPV) are best thought of as the clinical relevance of a test\n\nPrevalence is the number of cases in a defined population at a single point in time and is expressed as a decimal or a percentage.\nSensitivity is the percentage of true positives (e.g. 90% sensitivity = 90% of people who have the target disease will test positive).\nSpecificity is the percentage of true negatives (e.g. 90% specificity = 90% of people who do not have the target disease will test negative).\nAUROC is a performance metric for “discrimination”: it tells you about the model’s ability to discriminate between cases (positive examples) and non-cases (negative examples.) For a ranking use case AUC-ROC is a measure that indicates how good the model is at ranking cases based on a score (how likely any two cases are correctly ordered). 0.5 indicates that the model is no better than random at ranking and 1 indicates a perfect model.\nAUC-PRC is a measure that indicates how good the model is at minimising the tradeoff between precision and recall. A high AUC-PRC represents the model can achieve high recall and high precision at the same time."
  },
  {
    "objectID": "08-modelling.html#further-resources",
    "href": "08-modelling.html#further-resources",
    "title": "8  Modelling",
    "section": "8.7 Further resources",
    "text": "8.7 Further resources\nRules of Machine Learning: Best Practices for ML Engineering (Google Machine Learning Guides)\nErdemir, A., Mulugeta, L., Ku, J.P. et al. Credible practice of modeling and simulation in healthcare: ten rules from a multidisciplinary perspective. J Transl Med 18, 369 (2020). https://doi.org/10.1186/s12967-020-02540-4\nW. Hare, A. R. Rutherford, K. Vasarhelyi and The Complex Systems Modelling Group: Modelling in Healthcare, American Mathematical Society (2010)\nA Practical Guide to Maintaining Machine Learning in Production"
  },
  {
    "objectID": "09-evaluation.html",
    "href": "09-evaluation.html",
    "title": "9  Evaluation",
    "section": "",
    "text": "After a model has been created and validated, it should be evaluated to determine whether it can address the question you set out to answer. In some cases this can take the form of a formal prospective data study, where an independent dataset is collected, and the performance of the model on this dataset is assessed. Models that affect patient standard of care will be subject to ethical processes which should be considered upfront - see section on Ethics.\nExisting models, such as from other countries or jurisdictions, can also be evaluated against a local dataset, which is a good way to tell how useful someone else’s model may be on your own data.\nLags in data availability can also affect evaluation - a model that goes live today may not be able to be effectively evaluated until several months later.\nPossible social and business impacts of model deployment should have been considered in the governance process. Once a model has been deployed, actual impacts can be assessed through data collection, which can also include interviews with interested parties."
  },
  {
    "objectID": "09-evaluation.html#model-safety-false-negatives-false-positives",
    "href": "09-evaluation.html#model-safety-false-negatives-false-positives",
    "title": "9  Evaluation",
    "section": "9.1 Model safety (false negatives, false positives)",
    "text": "9.1 Model safety (false negatives, false positives)\nFrom a clinical perspective the main concern/clinical risk often concerns false negative/low-risk individuals with a positive outcome. Include clinical experts in the discussions on appropriate model thresholds.\nWhen models are implemented, people with high risk will often have additional things done for them to reduce the risk (less of a concern as we are actively doing something to reduce the risk). For the low-risk individuals, there are two possible explanations:\n\nThe outcome was unpredictable\nAdditional data could be used for prediction\n\nIt is usually difficult for a data scientist to know what additional data could have been used and it is helpful for the clinical lead to audit a number of false negative results to determine if the outcome was truly unpredictable or if additional data would have helped with the prediction."
  },
  {
    "objectID": "09-evaluation.html#bias-in-data",
    "href": "09-evaluation.html#bias-in-data",
    "title": "9  Evaluation",
    "section": "9.2 Bias in data",
    "text": "9.2 Bias in data\nData is collected in a particular context, which leads to bias. This can come from different sources including historical bias, data imbalance, missingness, and human prejudice. There is no single best definition of bias or fairness that applies equally well for every data science application.\nWhile training machine learning models using historically collected data, or drawing any conclusion from data, we should be typically mindful about the potential bias in the data regarding sensitive attributes such as age, ethnicity and gender. Bias-related harms can be reinforced by machine learning models/systems.\nMachine learning fairness itself is a broad topic. For a non-exhaustive summary of machine learning fairness from a technical perspective please refer to ML fairness. (Note Bellamy et al.)\nDue to the history focus of cohort studies, certain groups of the population, such as certain ethnic groups, females, might be under-represented and more vulnerable to bias in such studies while any conclusions were drawn or models were trained. In evaluation work, it is important to measure the goodness of fit, accuracy and other metrics of a model from multiple perspectives rather than the overall metrics only. Basic measurement aspects with respect to sensitive attributes (e.g. gender, ethnicity) to be considered:\n\nThe difference of actual patient data metrics stratified by sensitive attributes - whether there is any inequity among the stratified groups, and what bias it may bring into the models\nThe difference of predicted outcome metrics stratified by sensitive attributes - whether there is any inequity in model predictions among the stratified groups, and what downstream consequence this may cause\nThe difference of model performance metrics among the stratified groups - whether the model is treating the groups equally, and what downstream consequence this may cause\n\nReporting metrics with stratification by sensitive attributes whenever applicable can help maintain an equity lens more easily. Performance of the IBIS/Tyrer-Cuzick model of breast cancer risk by race and ethnicity in the Women's Health Initiative is an example (see Kurian et al.).\n\nKurian, Allison W., et al. “Performance of the IBIS/Tyrer‐cuzick Model of Breast Cancer Risk by Race and Ethnicity in the Women’s Health Initiative.” Cancer, vol. 127, no. 20, Oct. 2021, pp. 3742–50, https://doi.org/10.1002/cncr.33767.\nThere is a trade-off between data informativeness / model performance and fairness. Most bias mitigation methods cannot avoid playing with this balance. It is highly recommended to take into account the use case and follow-up impacts while deciding which bias mitigation method to be used and how to use it.\nMitigating bias and improving fairness is mostly not a technical challenge but a much broader systematic challenge.\nWe recommend including diverse voices and perspectives in data science work, e.g., having a Māori researcher(s) in the project.\nEven if no mitigation can be done, it is recommended that the bias itself should be analysed and reported if possible, especially that it is important to identify who is most vulnerable to the bias-related harms.\nTools:\n\nAequitas\nAudit-AI\nAI Fairness 360 (IBM)\nFairlearn (Microsoft)\nThe LinkedIn Fairness Toolkit (LiFT)\nFairness Indicator (Google)\nPROBAST\n\nFurther resources:\n\nFATE: https://www.microsoft.com/en-us/research/theme/fate/#!publications\nhttps://www.thinkwithgoogle.com/feature/ml-fairness-for-marketers/\nML fairness gym\nMinistry of Health - Emerging Health Technology Advice & Guidance"
  },
  {
    "objectID": "09-evaluation.html#clinician-in-the-loop",
    "href": "09-evaluation.html#clinician-in-the-loop",
    "title": "9  Evaluation",
    "section": "9.3 Clinician in the loop",
    "text": "9.3 Clinician in the loop\nexpand"
  },
  {
    "objectID": "09-evaluation.html#transparency-interpretability-and-explanation",
    "href": "09-evaluation.html#transparency-interpretability-and-explanation",
    "title": "9  Evaluation",
    "section": "9.4 Transparency, interpretability, and explanation",
    "text": "9.4 Transparency, interpretability, and explanation\nIn the context of health it is especially important to communicate findings in a way that builds trust in the model development process and outputs. If this is not done well, people may not adopt a model in practice that would otherwise have positive health impacts.\nThe requirement that models are transparent, interpretable and explainable may guide early modelling decisions such as which algorithm to use and how to treat inputs. Linear models tend to be more interpretable and explainable, so too are models that are built with inputs that have not been subject to significant transformations or weightings. The choice of a ‘simpler’ model may compromise model performance meaning better outcomes are sacrificed for the sake of explainability, however there is little point in deploying a deep learning model that has excellent performance, but is not trusted or used. This trade-off needs to be carefully considered.\nConsider outcome measures that are tangible and meaningful to the end user, and that have some relationship to the project goal, such as hospitalisation, mortality, or rankings.\nIndicate feature importance and how the model inputs are weighted in relation to model outputs and what that means in practice. For example, if a model includes modifiable inputs, a model user will want to know how changes to that input might affect an outcome. From an equity perspective, a model user will also want to know to what extent inputs such as ethnicity, age, gender or deprivation impact the outcome.\nThese considerations are also relevant to governance of models. GDPR’s regulation specifically emphasises a model’s transparency, accountability and governance (see (Kaminski and Malgieri)).\n\nKaminski, Margot E., and Gianclaudio Malgieri. “Algorithmic Impact Assessments Under the GDPR: Producing Multi-Layered Explanations.” International Data Privacy Law, vol. 11, no. 2, Aug. 2021, pp. 125–44, https://doi.org/10.1093/idpl/ipaa020.\n\n9.4.1 Transparency around the inputs\nProvide good data definitions and reasons for how data has been treated e.g “The model includes age but it has been grouped into 3 categories (18-39, 40-59 and 60+) to simplify the model and handle outliers without compromising performance”, and, “The count of regular medications includes medications that have been prescribed in the two years prior to test positive date for this infection. Regular medications are medications that have been prescribed at least four times over that period. Prescription data rather than dispensing data is used as it has better coverage.”\nProvide the provenance of data e.g. “This data came from a national collection of data that went through a quality assurance process, it is current and relevant to the problem being answered in this way…”\n\n\n9.4.2 Interpretation of the output\nThere are different types of output from different algorithms, e.g. risk scores, predictions, simulation results. In particular, users need to understand what the value means in the context of how it will be used. For example, a risk score of 0.8 might mean 80% probability of an outcome or it may mean something else depending on how the model is built and the use case. If the risk score has been developed for a ranking use case, the end user will need to understand that the output for a given person has meaning in relation to outputs for other people to determine who is at higher risk, rather than as a standalone value.\n\n\n9.4.3 Transparency of algorithm development\nAuditing the behaviour of an algorithm at the population level. For example, does the relationship between the predicted values and certain covariants (e.g. increased predicted mortality risk vs. age) for the validation cohort match with empirical evidence or the clinician’s cognition? Interpretation techniques such as partial dependence plots can facilitate such inspection.\nProviding individual level prediction reasoning - provide explanations of the prediction for a specific individual. For example, why the algorithm predicted a 0.86 readmission risk score for a 75 years old Pasifika woman. The score can be attributed to her age, previous hospitalisation history, cancer diagnosis, ethnicity and a few other risk factors. Shapley values is one of the commonly adopted techniques to provide explanations at the individual level. (Christoph Molnar) provides more details about model interpretation techniques.\n\nChristoph Molnar. Interpretable Machine Learning. 22 Oct. 2022, https://christophm.github.io/interpretable-ml-book/index.html.\nClinicians or other stakeholders need to be involved in the population and individual level algorithm auditing, and drive the iterations of the algorithm development with their feedback.\nWhere possible, making the code base and dataset public adds credibility\n\n\n9.4.4 Understanding the performance of an algorithm\nDefine performance metrics in the context of the problem. Plain language and accessible explanations not only help build trust in the model, they help the user understand how to use the model, encouraging adoption.\nTake, for example, a use case where a model is being used to predict a condition (with prevalence 2%) that requires an intervention. A positive predictive value of 0.95 means that of those people that presented with the condition, 95% were identified by the model. Putting this into context we could say that if 1000 people a day were assessed, we would expect that 20 of them would require the intervention. The model would identify 19 of those people, meaning weekly, 5 people with that condition would be missed if we were to rely solely on model outputs.\nInformation like this can help a clinician understand that while the model performs well, in practice they might like to supplement model outputs with other assessments.\nFor models that output probabilities, such as logistic regression, such analyses can be useful in helping clinicians quantify and understand the trade-off between false positives and false negatives in order to decide which decision thresholds may be appropriate.\n\n\n9.4.5 Understanding the impact of an algorithm\nEvaluate model benefit and cost in the context of realistic scenarios. With classic model performance metrics such RMSE, accuracy, precision or recall, it is often not enough to illustrate the consequences of integrating the algorithm into a healthcare workflow. It needs to be understood and documented how the algorithm would be integrated in a workflow, and is worth further evaluating what could be the potential healthcare outcome, especially when there is a clinical capacity limitation. For example, a model is developed to classify GP referred patients into high priority and low priority using a certain priority score threshold, and is targeted to facilitate timely triaging. However, in the system there are many people already waiting in the triage queue, people newly referred each day, and the number of triages the clinicians can process has a limit and uncertainty. By just looking at the classification metrics of this model, without knowing at which step of the process this model will be used and how, it is hard to tell exactly whether the integration of the model will bring more benefit than cost in the waiting time for patients who are in urgent need.\nDefining proper impact metrics according to the use case and goal of modelling, and carefully running through a further evaluation given the workflow will provide more informative insights than just the classic model performance metrics. Deterministic or stochastic simulation techniques can be applied for such evaluation when a working scenario can be quantitatively described. As a straightforward example, the New Zealand business case for hospital avoidance programme using a readmission risk model presented its financial impact in healthcare (Vaithianathan et al. 2012).\n\n\n9.4.6 Principles to follow\n\nClosely engage with the stakeholders and data providers (covered in End-user engagement)\nKeep clinicians in the loop (Clinician in the loop section heading added)\nHave good quality documentation to share the work with others. Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis (TRIPOD) provides a well established and practical template for healthcare model reporting. For simpler reporting, consider Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist as an alternative. For key information from governance perspective, refer to the governance process and Algorithm Information Request template for the New Zealand Algorithm Hub (https://algorithmhub.co.nz/about).\nVersion control the code base so that work is reproducible\nData quality checking: availability including operational concerns, sanity checking, bias\nEquity concerns: who benefits from the algorithm; who may be vulnerable\nConsider your audience when presenting data"
  },
  {
    "objectID": "10-deployment-lifecycle.html",
    "href": "10-deployment-lifecycle.html",
    "title": "10  Deployment & Lifecycle Maintenance",
    "section": "",
    "text": "Health data science projects often aim to produce tools that are intended to be used in decision support - through presenting insights or suggestions. Thinking through the implications of the intended use in practice is critical to success.\nWe recommend planning for this from the start if operational deployment is an explicit goal as there is a big difference between research and operationalising something. Engage early with SMEs to gain the right mix of expertise for a successful implementation.\nSome questions worth considering include:\n\nWho are the users, and what knowledge/capability/training will they need?\nIs all the relevant data available at the point it is intended to be used, and is it current?\nIs it feasible to get operational data supply ongoing to support this?\nHow will models/decisions/insights be presented to the user? Can users understand how the model came to a conclusion and what action is required? See section on Transparency and Interpretability\nWhat monitoring, IT security and safeguards are likely to be necessary?\nWhat systems will be utilised, and who is responsible for them?\nWho will provide governance oversight of the model?\nWill the model be updated when new data becomes available?\nWhat exactly is expected to be deployed? An API? A frontend?\nHow are the models and API going to be versioned?\nHow much traffic are you expecting?\nHow much response time and performance in time is acceptable ?\nWhat level of work is required from warehouse developers when developing models? - a simple model is more likely to be implemented than one which is complicated even if there is a performance decrease.\n\nThe below list outlines ideas and recommendations concerning operational deployment of ML solutions within the medical sector:\n\nDraw clear lines between research, analytics and software development. If you are looking to offer software as a service, then you need software engineers who are familiar with software development practices (e.g. writing production-grade code, developing infrastructure) that are necessary to succeed.\nKeep things simple: most prospective customers in the medical sector are conservative and they need to understand (at least to some extent) solutions that are being offered to them and they are also most probably using outdated solutions (as opposed to cutting edge research)\nMany prospective customers in the medical sector are going to be unwilling to send their data to an API over the public internet. You may either need to specifically focus on the niche of customers who do not have these constraints or you need to develop solutions that can be deployed on premise. This has profound implications on your architectural decisions (e.g. federated learning), your ability to provide maintenance and support as well as your ability to keep tight control over your source code\nIdentifiable health data is regulated around the world and you need to ensure that you follow the right security practices for your clients to be able to use your services. This also has downstream implications on debugging, logging, performance tracking, etc\nMuch of the publicly available health data that can be used to train models that you may wish to deploy in production have licences that forbid commercial usage – make sure you check these constraints before committing to a project\nIf you are offering ML models to external clients and your models leverage open source data, then you will find that you don’t need to regularly retrain these models since you’ll most likely be using a static snapshot for a one-off training exercise. This simplifies model management\nFor IT implementation at partner/clinical sites, typically design, governance, and security sign-off will be required before people can be allocated to the work. This requires clear articulation of the benefits of the work in order to be prioritised. Ensure there is strong clinical engagement from the partner site to drive this see End-user Engagement and collaboration\nAlso refer to the Ministry of Health - Emerging Health Technology Advice & Guidance on operationalising algorithms"
  },
  {
    "objectID": "10-deployment-lifecycle.html#lifecycle-model-maintenance",
    "href": "10-deployment-lifecycle.html#lifecycle-model-maintenance",
    "title": "10  Deployment & Lifecycle Maintenance",
    "section": "10.2 Lifecycle model maintenance",
    "text": "10.2 Lifecycle model maintenance\n\n\n\nThe CRISP-DM Data Science Lifecycle\n\n\nShearer, C. (2000). The CRISP-DM model: the new blueprint for data mining. Journal of data warehousing, 5(4), 13-22.\nDeployment of a model is never the end of modelling work. During its continuing life cycle, the model and its working environment need to be overseen continuously. Some early questions to consider are:\n\nWhat is the governance process?\nWho is responsible for oversight and/or maintenance?\nWhat is the process for monitoring model drift?\nIs the model being used in the way it was originally intended according to the original use case?\nDo the intended users of the model understand, trust and use the model outputs?\n\nIn the case of “model drift” (non-negligible performance decrease is detected, or any condition for the model to properly work is no longer satisfied), the model and model application need to be reviewed, and in necessary cases the partial or whole modelling process should be reapplied to create a refreshed model.\nRefer also to ’explaining the outputs, transparency & interpretability\n\n10.2.1 Model refreshment\nThere are two main approaches for triggering model refreshment:\n\nTime based refreshment. Retrain the model (regardless of the performance) at a regular interval. Better quality data, more current data, data with better population coverage or new data sources may become available over time meaning that a model trained on updated data would likely have significant performance improvements over the existing model. A good understanding of how frequently the data changes is needed for this approach.\nPerformance based refreshment. Continuously monitor a set of the model performance metrics (and/or other metrics such as bias) to determine when the model needs a retraining. A good selection of the panel of metrics and thresholds is needed for this approach.\n\nBut there are other considerations for model refreshment including:\n\nAny change in how the model is being used. Are the users of the model still using it according to the original use case? If the model is being used for a different purpose, is it appropriate for that purpose?\nPotential for increased scrutiny of the model. You don’t want your model to be viewed negatively publicly and should consider how use of the model would look on the front page of the newspaper, particularly if there have been shifts in the inputs, in the outcomes, in the environment (e.g. new disease variants) or in attitudes that affect social licence.\n\n\n\n10.2.2 Model monitoring\nKey areas to be monitored during model use include:\n\nThe data feed. Are there any changes in the statistical properties of the data that are fed into the model, and the actual data labels or patient outcomes (there will be a lag to collect these) that the model predicts? Are they still similar to the data that the model was trained on? Has a “concept drift” detector been set up in the monitoring process? How is the bias in the data changing?\nThe model performance metrics and bias metrics. Is there any significant drop in the model performance or model fairness and what are the possible reasons to be taken into account for retraining? This could be when a given metric, such as accuracy or combination of metrics, such a sensitivity and specificity have dropped below a certain threshold, or there is a clear downward trend in those metrics.\nThe key assumptions that the modelling was based on. For example, in the COVID pandemic modelling work, is the current dominating virus variant still the same as the one when the modelling data were collected?\nThe workflow where the model is integrated. Are there any changes in the upstream or downstream steps of the workflow that may make the model not applicable any more?\nThe risk, benefit and cost of model usage. Does the benefit outweigh the risk in real use? Is the model use case as cost-effective as it was expected at design?\n\nMore can be done besides the list above. For a more comprehensive model monitoring and retraining techniques, refer to this blog: A Practical Guide to Maintaining Machine Learning in Production."
  },
  {
    "objectID": "11-references.html",
    "href": "11-references.html",
    "title": "Useful references and templates",
    "section": "",
    "text": "Note: Move these to the right sections in the document, rather than at the end."
  },
  {
    "objectID": "11-references.html#references-from-citations",
    "href": "11-references.html#references-from-citations",
    "title": "Useful references and templates",
    "section": "References from citations",
    "text": "References from citations"
  }
]